# Impaired goal-directed planning in transdiagnostic compulsivity is explained by uncertainty about learned task structure

_Sookud, Martin, Gillan & Wise (2024)_

ðŸ”— [Preprint](https://osf.io/preprints/psyarxiv/zp6vk)

This repository contains analysis code for the project looking at how transition learning relates to symptoms of mental health problems.

## ðŸ’¾ Installing the code as a Python package

### Python version

The code is written in **Python 3.9**. It is recommended to use a Python 3.9 environment to run the code. You can check your Python version by running:

```bash
python --version
```

And create a Python 3.9 environment using [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) with:

```bash
conda create -n python39 python=3.9
conda activate python39
```

(here `python39` is the name of the environment, which can be changed to any name you prefer)

### Installation

The custom-written code for the project is contained withing the `./transition_uncertainty` directory, and can be installed as a Python package.

[Clone](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository#cloning-a-repository) or [download](https://docs.github.com/en/repositories/working-with-files/using-files/downloading-source-code-archives#downloading-source-code-archives) this repository, navigate to the root directory of the repository and run:

```bash
pip install -e .
```

For example:

```bash
git clone https://github.com/the-wise-lab/transition-learning-public.git
cd transition-learning-public
pip install -e .
```

The project-specific code is then accessible through the usual import mechanism, e.g.:

```python
from transition_uncertainty.analysis_utils import calculate_intraclass_correlations
```

This code is project-specific, and is not intended to be used outside of this project as it refers to directories and files that are specific to this project.

> [!IMPORTANT]
>
> Setup for this package involves installation of dependency packages with specific versions to ensure replicability. These are relatively up to date as of now (2024), but will become outdated in the future. It may be best to install this package within a virtual environment to avoid conflicts with other packages that are more up to date.

> [!IMPORTANT]
 Some of the code used in this project makes use of [Jax](https://jax.readthedocs.io/), which may not run run on Windows. As such, this code can only be assumed to work properly on Linux/MacOS (or Windows using [WSL](https://docs.microsoft.com/en-us/windows/wsl/install-win10))

### ðŸ““ Running notebooks

You will also need to be able to run [Jupyter notebooks](https://jupyter.org/install) to run the analysis notebooks. One option is to install [JupyterLab](https://jupyter.org/install) using pip:

```bash
pip install jupyterlab
```

## ðŸ—‚ï¸ Repository structure

The repository is structured as follows:

```
Repository Structure:
â”œâ”€â”€ data                    # Raw data files
â”‚   â”œâ”€â”€ discovery           # Data for the discovery sample
â”‚   â”œâ”€â”€ replication         # Data for the replication sample
â”‚   â”œâ”€â”€ retest              # Data for the replication sample
â”‚   â”œâ”€â”€ follow-up           # Data for the follow-up of the discovery sample
â”‚   â””â”€â”€ follow-up-1yr       # Data for the follow-up of the discovery sample after 1 year
â”œâ”€â”€ figures                 # Figures generated by the code
â”œâ”€â”€ models                  # Pre-trained models for the analyses
â”œâ”€â”€ notebooks               # Jupyter notebooks used for analyses
â”œâ”€â”€ results                 # Processed data and final results
â”‚   â”œâ”€â”€ discovery           # Results for the discovery sample
â”‚   â”œâ”€â”€ replication         # Results for the replication sample
â”‚   â”œâ”€â”€ retest              # Results for the replication sample
â”‚   â”œâ”€â”€ follow-up           # Results for the follow-up of the discovery sample
|   â””â”€â”€ follow-up-1yr       # Results for the follow-up of the discovery sample after 1 year
â”œâ”€â”€ scripts                 # Scripts used to run analyses
â””â”€â”€ transition_uncertainty  # The package described above
```

The `discovery` and `replication`, `retest`, and `follow-up` subdirectories within `data` refer to the four different datasets used for the project:

1. `discovery` - the main discovery sample, used for the main analyses.
2. `replication` - the replication sample, used to replicate the main analyses.
3. `retest` - the retest sample, used to investigate the test-retest reliability of the task and questionnaires over a 2 week period (taken from the `replication` sample).
4. `follow-up` - the follow-up sample, used to investigate how changes in behaviour relate to changes in mental health over a 3 month period (taken from the `discovery` sample).
5. `follow-up-1yr` - the follow-up sample after 1 year, used to investigate how changes in behaviour relate to changes in mental health over a 1 year period (taken from the `discovery` sample).

These directories are organised as follows:

```
â”œâ”€â”€ transition-task     # Data for the main transition learning task
â”œâ”€â”€ questionnaires      # Data from questionnaires (Not present for retest sample)
â””â”€â”€ twostep-task        # Data for the two-step task (Replication and retest samples only)
```

The corresponding directories within the `results` directory are organised as follows:

```
â”œâ”€â”€ transition-task_model-fit     # Model fitting results for the main transition learning task
â”œâ”€â”€ confidence_model-fit          # Results of the confidence regression models
â”œâ”€â”€ transdiagnostic-factors       # Estimated transdiagnostic factor scores
â””â”€â”€ two-step_model-fit            # Results of two-step task modelling (Replication and retest samples only)
```

The `./figures` directory will be created when running the notebooks, and will contain the figures generated by the code. The `./results` and `./data` directories can be downloaded separately (see below).

## ðŸ“¥ Downloading data

The `./data`, `models`, and `./results` directories are not included in the repository due to their size.

Data and results for the project are stored on the Open Science Framework (OSF) [here](https://osf.io/a7e3k). For convenience, the data can be downloaded using the `download_data.py` script. This script will download and extract the data to the `./data` directory, SBI models to the `models` directory, and results to the `./results` directory (this might take a while due to the file size). It can be run from the command line as follows:

```bash
python download_data.py
```
> [!NOTE]
This will download the original results for all of the analyses and store them in `./models` and `./results`. These can be overwritten by running the notebooks, which will save the results to the same directory.

## ðŸ”¬ Reproducing the analyses

All of the analysis steps are run within Jupyter notebooks. These should be performed in the order given below.

Notebooks are located within the `notebooks` directory, and are divided into subdirectories for the discovery, replication, and follow-up samples. 

### 1. Computational model fitting

Model fitting is performed using simulation based inference, using a custom [simulation based inference package](https://github.com/tobywise/simulation_based_inference). Pre-trained models are included in the `./results` directory. To re-fit the models, use the scripts provided in the `./scripts` directory.

Model fitting using the pre-trained models is performed in the `Model fitting.ipynb` notebooks. This runs the model fitting procedure, estimates parameter values for each participant, and stores the results in the `./results` directory. It also estimates model fit using the WAIC method, allowing for model comparison and providing a measure of model fit for each participant.

> [!TIP]
The model fitting procedures contained in `scripts/train_sbi.py` will run faster on GPUs. The model comparison procedures are also relatively memory intensive, and may not run on machines with limited memory.

### 2. Analyses of confidence reports

We fit hierarchical Bayesian models to estimate the influence of learning parameters on confidence reports in the task. This is performed in the `Confidence analysis.ipynb` notebooks.

### 3. Two-step task modelling

Participants in the replication sample also completed a gamified version of the original "two-step' task, as described by [Donegan et al. (2023)](https://www.nature.com/articles/s44271-023-00031-y). Analysis of this data is performed in the `Two-step modelling.ipynb` notebooks.

### 4. Estimating scores on transdiagnostic factors

We used a reduced set of items to measure the three factors identified by [Gillan et al. (2016)](https://elifesciences.org/articles/11305). We use the [FACSIMILE](https://github.com/tobywise/FACSIMILE) method to estimate true factor scores for these three factors. This is performed in the `Transdiagnostic factors.ipynb` notebooks.

### 5. Analyses of relationships between behaviour and mental health

Next, we perform analyses to investigate the relationship between behaviour in the task and mental health. This is performed in the `Symptom analyses.ipynb` notebooks.

### 6. Test-retest reliability

We look at test-retest reliability of the task over a 2-week period in the `retest` and `replication` samples. This is performed in the `Test-retest.ipynb` notebooks.

### 7. Longitudinal analyses

A subset of participants from the discovery sample were followed up after 3 months and 1 year, where they completed the transition learning task alongside questionnaires for a second time. We perform analyses to investigate how changes in behaviour relate to changes in mental health over this period. This is performed in the `Longitudinal analyses.ipynb` notebooks.

### âœ” Running all analyses

We advise running the notebooks described above in the following sequence:

#### Discovery sample

1. Model fitting: [`Model fitting - Discovery.ipynb`](</notebooks/discovery/Model fitting - Discovery.ipynb>)
2. Confidence analysis: [`Confidence analysis - Discovery.ipynb`](</notebooks/discovery/Confidence analysis - Discovery.ipynb>)
3. Transdiagnostic factor estimation: [`Transdiagnostic factors - Discovery.ipynb`](</notebooks/discovery/Transdiagnostic factors - Discovery.ipynb>)
4. Symptom-behaviour analyses: [`Symptom analyses - Discovery.ipynb`](</notebooks/discovery/Symptom analyses - Discovery.ipynb>)

#### Replication sample

1. Model fitting: [`Model fitting - Replication.ipynb`](</notebooks/replication/Model fitting - Replication.ipynb>)
2. Confidence analysis: [`Confidence analysis - Replication.ipynb`](</notebooks/replication/Confidence analysis - Replication.ipynb>)
3. Two-step task analysis: [`Two-step modelling - Replication.ipynb`](</notebooks/replication/Two-step modelling - Replication.ipynb>)
4. Transdiagnostic factor estimation: [`Transdiagnostic factors - Replication.ipynb`](</notebooks/replication/Transdiagnostic factors - Replication.ipynb>)
5. Symptom-behaviour analyses: [`Symptom analyses - Replication.ipynb`](</notebooks/replication/Symptom analyses - Replication.ipynb>)

#### Test-retest sample

1. Model-fitting: [`Model fitting - Retest.ipynb`](</notebooks/retest/Model fitting - Retest.ipynb>)
2. Two-step modelling: [`Two-step modelling - Retest.ipynb`](</notebooks/retest/Two-step modelling - Retest.ipynb>)
3. Test-retest reliability analyses: [`Test-retest - Retest.ipynb`](</notebooks/retest/Test-retest - Retest.ipynb>)

#### Follow-up sample

1. Model fitting: [`Model fitting - Follow up.ipynb`](</notebooks/follow-up/Model fitting - Follow up.ipynb>)
2. Transdiagnostic factor estimation: [`Transdiagnostic factors - Follow up.ipynb`](</notebooks/follow-up/Transdiagnostic factors - Follow up.ipynb>)
3. Test-retest reliability analyses: [`Test-retest - Follow up.ipynb`](</notebooks/follow-up/Test-retest - Follow up.ipynb>)
4. Longitudinal analyses: [`Longitudinal analyses - Follow up.ipynb`](</notebooks/follow-up/Longitudinal analyses - Follow up.ipynb>)

#### Follow-up sample (1 year)

1. Model fitting: [`Model fitting - Follow up 1yr.ipynb`](</notebooks/follow-up-1yr/Model fitting - Follow up 1yr.ipynb>)
2. Transdiagnostic factor estimation: [`Transdiagnostic factors - Follow up 1yr.ipynb`](</notebooks/follow-up-1yr/Transdiagnostic factors - Follow up 1yr.ipynb>)
3. Test-retest reliability analyses: [`Test-retest - Follow up 1yr.ipynb`](</notebooks/follow-up-1yr/Test-retest - Follow up 1yr.ipynb>)
4. Longitudinal analyses: [`Longitudinal analyses -  Follow up 1yr.ipynb`](</notebooks/follow-up-1yr/Longitudinal analyses - Follow up 1yr.ipynb>)

## âž• Additional analyses

### Parameter and model recovery

Notebooks are provided that run model and parameter recovery tests:

* [`Model recovery.ipynb`](</notebooks/Model recovery.ipynb>)
* [`Parameter recovery.ipynb`](</notebooks/Parameter recovery.ipynb>)

Alongside a notebook that generates figures for the paper:

* [`Figures.ipynb`](</notebooks/Figures.ipynb>)
